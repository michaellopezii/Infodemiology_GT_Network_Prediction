{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_directional_dtw(google_trends: np.ndarray, disease: np.ndarray, radius: int) -> Tuple[float, List]:\n",
    "    \"\"\"\n",
    "    Compute DTW from network metrics perspective (one point to many)\n",
    "    \"\"\"\n",
    "    # Normalize series to [0,1] range\n",
    "    gt_norm = (google_trends - np.min(google_trends)) / (np.max(google_trends) - np.min(google_trends))\n",
    "    disease_norm = (disease - np.min(disease)) / (np.max(disease) - np.min(disease))\n",
    "    \n",
    "    n = len(gt_norm)  # Length of network metric series\n",
    "    m = len(disease_norm)  # Length of disease series\n",
    "    \n",
    "    # Initialize cost and path matrices\n",
    "    D = np.full((n, m), np.inf)\n",
    "    paths = [[[] for _ in range(m)] for _ in range(n)]\n",
    "    \n",
    "    # For each point in network metric series\n",
    "    for i in range(n):\n",
    "        # Find valid range in disease series within radius\n",
    "        start_j = max(0, i - radius)\n",
    "        end_j = min(m, i + radius + 1)\n",
    "        \n",
    "        # Find all matches within radius\n",
    "        for j in range(start_j, end_j):\n",
    "            cost = abs(gt_norm[i] - disease_norm[j])\n",
    "            if i == 0:\n",
    "                D[i, j] = cost\n",
    "                paths[i][j] = [(i, j)]\n",
    "            else:\n",
    "                # Get minimum cost from previous network metric point\n",
    "                prev_costs = D[i-1, max(0, j-radius):min(m, j+radius+1)]\n",
    "                min_prev_cost = np.min(prev_costs) if len(prev_costs) > 0 else np.inf\n",
    "                if min_prev_cost != np.inf:\n",
    "                    D[i, j] = cost + min_prev_cost\n",
    "                    # Get path from previous point with minimum cost\n",
    "                    prev_j = max(0, j-radius) + np.argmin(prev_costs)\n",
    "                    paths[i][j] = paths[i-1][prev_j] + [(i, j)]\n",
    "    \n",
    "    # Find best end point\n",
    "    final_row = D[n-1, :]\n",
    "    best_end = np.argmin(final_row)\n",
    "    best_cost = final_row[best_end]\n",
    "    best_path = paths[n-1][best_end]\n",
    "    \n",
    "    return best_cost, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_directional_dtw(network_metric: np.ndarray, disease: np.ndarray, dates: np.ndarray,\n",
    "                        metric_type: str, source: str, threshold: str, window: str,\n",
    "                        comparison: str, radius: int) -> None:\n",
    "    \"\"\"\n",
    "    Plot DTW alignment showing directional matching\n",
    "    \"\"\"\n",
    "    # Normalize series\n",
    "    net_norm = (network_metric - np.min(network_metric)) / (np.max(network_metric) - np.min(network_metric))\n",
    "    disease_norm = (disease - np.min(disease)) / (np.max(disease) - np.min(disease))\n",
    "    \n",
    "    # Compute DTW\n",
    "    distance, path = compute_directional_dtw(network_metric, disease, radius)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    # Plot time series\n",
    "    ax.plot(dates, disease_norm + 1.5, label=f'{comparison}', color='blue', linewidth=2)\n",
    "    ax.plot(dates, net_norm, label=f'{metric_type} ({source}, thresh={threshold})', color='red', linewidth=2)\n",
    "    \n",
    "    # Draw matching lines every 2 days\n",
    "    path = np.array(path)\n",
    "    for idx, (i, j) in enumerate(path):\n",
    "        if i % 2 == 0:\n",
    "            ax.plot([dates[i], dates[j]], [net_norm[i], disease_norm[j] + 1.5], \n",
    "                    'gray', alpha=0.9, linestyle='--')\n",
    "    \n",
    "    # Convert dates to pandas Timestamp for string formatting\n",
    "    start_date = pd.Timestamp(dates[0]).strftime('%Y-%m-%d')\n",
    "    end_date = pd.Timestamp(dates[-1]).strftime('%Y-%m-%d')\n",
    "    timeline_info = f\"{start_date} to {end_date}\"\n",
    "    \n",
    "    ax.set_title(f'Directional DTW Alignment (Â±{radius} days): {metric_type}\\n{source} (), threshold={threshold}', \n",
    "                 fontsize=25, pad=20)\n",
    "    ax.legend(fontsize=13, loc='upper right')\n",
    "    \n",
    "    # Remove y-axis ticks and labels\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Format x-axis\n",
    "    formatter = mdates.DateFormatter('%B %Y')\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    ax.tick_params(axis='x', labelsize=15)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Set x-axis limits with padding\n",
    "    date_range = dates[-1] - dates[0]\n",
    "    padding = pd.Timedelta(days=(date_range / pd.Timedelta('1D')) * 0.05)  # 5% padding\n",
    "    ax.set_xlim(dates[0] - padding, dates[-1] + padding)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    filename = f\"dtw_directional_{metric_type.lower()}_{source.lower()}_{threshold}_{window}day_{comparison.lower()}_r{radius}.png\"\n",
    "    plt.savefig(filename.replace('&', 'and'), bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_network_metrics(base_path: str, disease_dfs: Dict[str, pd.DataFrame], \n",
    "                          radii: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze network metrics using directional DTW\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Process network density files\n",
    "    density_path = os.path.join(base_path, \"gt_netdense_rsvmsv_15or30day\")\n",
    "    pattern = \"netdense_*.csv\"\n",
    "    density_files = glob.glob(os.path.join(density_path, pattern))\n",
    "    \n",
    "    for file_path in density_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        _, source, threshold, window = filename.replace('.csv', '').split('_')\n",
    "        window = window.replace('day', '')\n",
    "        \n",
    "        print(f\"\\nProcessing Network Density: {filename}\")\n",
    "        \n",
    "        # Load network density data\n",
    "        metric_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Skip if all zeros\n",
    "        if np.all(metric_df['network_density'] == 0):\n",
    "            print(f\"Skipping {filename} - contains all zeros\")\n",
    "            continue\n",
    "            \n",
    "        metric_df['date'] = pd.to_datetime(metric_df['date'])\n",
    "        \n",
    "        # Compare with each disease metric\n",
    "        for disease_name, disease_df in disease_dfs.items():\n",
    "            # Filter disease data to match network metric timeframe\n",
    "            disease_filtered = disease_df[\n",
    "                (disease_df['date'] >= metric_df['date'].min()) &\n",
    "                (disease_df['date'] <= metric_df['date'].max())\n",
    "            ].copy()\n",
    "            \n",
    "            # Ensure date alignment\n",
    "            merged_df = pd.merge(metric_df, disease_filtered, on='date', how='inner')\n",
    "            \n",
    "            if len(merged_df) == 0:\n",
    "                print(f\"No overlapping dates found for {filename} and {disease_name}\")\n",
    "                continue\n",
    "                \n",
    "            metric_series = merged_df['network_density'].values\n",
    "            disease_series = merged_df.iloc[:, -1].values\n",
    "            dates = merged_df['date'].values\n",
    "            \n",
    "            # Compute DTW for each radius\n",
    "            for radius in radii:\n",
    "                try:\n",
    "                    dtw_score, _ = compute_directional_dtw(\n",
    "                        metric_series, disease_series, radius\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        'metric_type': 'Network Density',\n",
    "                        'source': source.upper(),\n",
    "                        'threshold': threshold,\n",
    "                        'window': window,\n",
    "                        'comparison': disease_name,\n",
    "                        'radius': radius,\n",
    "                        'dtw_score': dtw_score\n",
    "                    })\n",
    "                    \n",
    "                    # Generate visualization\n",
    "                    plot_directional_dtw(metric_series, disease_series, dates,\n",
    "                                       'Network Density', source.upper(), threshold,\n",
    "                                       window, disease_name, radius)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename} with radius {radius}: {str(e)}\")\n",
    "    \n",
    "    # Process clustering coefficient files\n",
    "    coeff_path = os.path.join(base_path, \"gt_cluscoeff_rsvmsv_15or30day\")\n",
    "    pattern = \"cluscoeff_*.csv\"\n",
    "    coeff_files = glob.glob(os.path.join(coeff_path, pattern))\n",
    "    \n",
    "    for file_path in coeff_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        _, source, threshold, window = filename.replace('.csv', '').split('_')\n",
    "        window = window.replace('day', '')\n",
    "        \n",
    "        print(f\"\\nProcessing Clustering Coefficient: {filename}\")\n",
    "        \n",
    "        # Load clustering coefficient data\n",
    "        metric_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Skip if all zeros\n",
    "        if np.all(metric_df['clustering_coefficient'] == 0):\n",
    "            print(f\"Skipping {filename} - contains all zeros\")\n",
    "            continue\n",
    "            \n",
    "        metric_df['date'] = pd.to_datetime(metric_df['date'])\n",
    "        \n",
    "        # Compare with each disease metric\n",
    "        for disease_name, disease_df in disease_dfs.items():\n",
    "            # Filter disease data to match clustering coefficient timeframe\n",
    "            disease_filtered = disease_df[\n",
    "                (disease_df['date'] >= metric_df['date'].min()) &\n",
    "                (disease_df['date'] <= metric_df['date'].max())\n",
    "            ].copy()\n",
    "            \n",
    "            # Ensure date alignment\n",
    "            merged_df = pd.merge(metric_df, disease_filtered, on='date', how='inner')\n",
    "            \n",
    "            if len(merged_df) == 0:\n",
    "                print(f\"No overlapping dates found for {filename} and {disease_name}\")\n",
    "                continue\n",
    "                \n",
    "            metric_series = merged_df['clustering_coefficient'].values\n",
    "            disease_series = merged_df.iloc[:, -1].values\n",
    "            dates = merged_df['date'].values\n",
    "            \n",
    "            # Compute DTW for each radius\n",
    "            for radius in radii:\n",
    "                try:\n",
    "                    dtw_score, _ = compute_directional_dtw(\n",
    "                        metric_series, disease_series, radius\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        'metric_type': 'Clustering Coefficient',\n",
    "                        'source': source.upper(),\n",
    "                        'threshold': threshold,\n",
    "                        'window': window,\n",
    "                        'comparison': disease_name,\n",
    "                        'radius': radius,\n",
    "                        'dtw_score': dtw_score\n",
    "                    })\n",
    "                    \n",
    "                    # Generate visualization\n",
    "                    plot_directional_dtw(metric_series, disease_series, dates,\n",
    "                                       'Clustering Coefficient', source.upper(), threshold,\n",
    "                                       window, disease_name, radius)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename} with radius {radius}: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df = pd.read_csv(\"../gt_stat_analysis/disease_confirmed_daily_cases.csv\")\n",
    "active_df = pd.read_csv(\"../gt_stat_analysis/disease_active_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df['date'] = pd.to_datetime(confirmed_df['date'])\n",
    "active_df['date'] = pd.to_datetime(active_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_dfs = {\n",
    "    'Confirmed Cases': confirmed_df,\n",
    "    'Active Cases': active_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = [7, 15, 20, 30, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../gt_netdense_cluscoeff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing network metrics\n",
      "\n",
      "Processing Network Density: netdense_msv_0.5_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.8_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.6_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.4_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.4_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.6_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.8_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.5_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.8_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.5_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.4_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.6_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.6_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.4_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.5_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.8_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.8_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.5_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.4_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.6_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.6_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.4_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.5_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.8_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.5_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.8_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.6_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.4_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.4_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.6_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.8_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.5_30day.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing network metrics\")\n",
    "all_results = analyze_network_metrics(base_path, disease_dfs, radii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in all_results.columns if col != 'dtw_score'] + ['dtw_score']\n",
    "all_results = all_results[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted = all_results.sort_values('dtw_score', ascending=True)\n",
    "results_sorted.to_csv('network_directional_dtw_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
