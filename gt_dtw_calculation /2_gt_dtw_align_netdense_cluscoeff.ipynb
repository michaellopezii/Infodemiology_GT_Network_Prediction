{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "from scipy.spatial.distance import euclidean\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy.ma as ma\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_constraint_mask(length: int, radius: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a mask matrix for temporal constraints.\n",
    "    Points outside the radius window will be masked with True\n",
    "    \"\"\"\n",
    "    mask = np.zeros((length, length), dtype=bool)\n",
    "    \n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if abs(i - j) > radius:\n",
    "                mask[i, j] = True\n",
    "                \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dtw_with_temporal_constraint(series1: np.ndarray, series2: np.ndarray, radius: int) -> Tuple[float, List]:\n",
    "    \"\"\"\n",
    "    Compute DTW with strict temporal constraints\n",
    "    \"\"\"\n",
    "    # Normalize series to [0,1] range\n",
    "    s1_norm = (series1 - np.min(series1)) / (np.max(series1) - np.min(series1))\n",
    "    s2_norm = (series2 - np.min(series2)) / (np.max(series2) - np.min(series2))\n",
    "    \n",
    "    n, m = len(s1_norm), len(s2_norm)\n",
    "    \n",
    "    # Create cost matrix\n",
    "    cost_matrix = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            cost_matrix[i, j] = abs(s1_norm[i] - s2_norm[j])\n",
    "    \n",
    "    # Create accumulated cost matrix with temporal constraint\n",
    "    D = np.full((n, m), np.inf)\n",
    "    D[0, 0] = cost_matrix[0, 0]\n",
    "    \n",
    "    # Create temporal constraint mask\n",
    "    temporal_mask = create_temporal_constraint_mask(max(n, m), radius)\n",
    "    temporal_mask = temporal_mask[:n, :m]\n",
    "    \n",
    "    # Apply temporal constraint\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if temporal_mask[i, j]:\n",
    "                continue\n",
    "                \n",
    "            if i == 0 and j == 0:\n",
    "                continue\n",
    "                \n",
    "            candidates = []\n",
    "            if i > 0:\n",
    "                candidates.append(D[i-1, j])\n",
    "            if j > 0:\n",
    "                candidates.append(D[i, j-1])\n",
    "            if i > 0 and j > 0:\n",
    "                candidates.append(D[i-1, j-1])\n",
    "            \n",
    "            if candidates:\n",
    "                D[i, j] = cost_matrix[i, j] + min(candidates)\n",
    "    \n",
    "    # Backtrack to find the warping path\n",
    "    path = []\n",
    "    i, j = n-1, m-1\n",
    "    path.append((i, j))\n",
    "    \n",
    "    while i > 0 or j > 0:\n",
    "        candidates = []\n",
    "        if i > 0:\n",
    "            candidates.append((D[i-1, j], i-1, j))\n",
    "        if j > 0:\n",
    "            candidates.append((D[i, j-1], i, j-1))\n",
    "        if i > 0 and j > 0:\n",
    "            candidates.append((D[i-1, j-1], i-1, j-1))\n",
    "            \n",
    "        _, i, j = min(candidates, key=lambda x: x[0])\n",
    "        path.append((i, j))\n",
    "    \n",
    "    path.reverse()\n",
    "    \n",
    "    return D[-1, -1], path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dtw_alignment(series1: np.ndarray, series2: np.ndarray, dates: np.ndarray,\n",
    "                      metric_type: str, value_type: str, source: str, threshold: str,\n",
    "                      window: str, comparison: str, radius: int) -> None:\n",
    "    \"\"\"\n",
    "    Plot the DTW alignment between network metrics and disease cases\n",
    "    \"\"\"\n",
    "    # Normalize series for visualization\n",
    "    s1_norm = (series1 - np.min(series1)) / (np.max(series1) - np.min(series1))\n",
    "    s2_norm = (series2 - np.min(series2)) / (np.max(series2) - np.min(series2))\n",
    "    \n",
    "    # Compute DTW with temporal constraint\n",
    "    distance, path = compute_dtw_with_temporal_constraint(series1, series2, radius)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    # Plot the first time series at the top\n",
    "    ax.plot(dates, s1_norm + 1.5, label=f'{comparison}', color='blue', linewidth=2)\n",
    "    \n",
    "    # Plot the second time series at the bottom\n",
    "    metric_label = f\"{value_type} ({source}, thresh={threshold}, window={window})\"\n",
    "    ax.plot(dates, s2_norm, label=metric_label, color='red', linewidth=2)\n",
    "    \n",
    "    # Draw matching lines between points\n",
    "    path = np.array(path)\n",
    "    for i, j in path[::3]:  # Plot every 3rd line to reduce visual clutter\n",
    "        ax.plot([dates[i], dates[j]], [s1_norm[i] + 1.5, s2_norm[j]], \n",
    "                'gray', alpha=0.9, linestyle='--')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title(f'DTW Alignment (Â±{radius} days): {metric_type}\\n{value_type} vs {comparison}', \n",
    "                 fontsize=25, pad=20)\n",
    "    ax.legend(fontsize=13, loc='upper right')\n",
    "    \n",
    "    # Remove y-axis ticks and labels\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Format x-axis dates\n",
    "    formatter = mdates.DateFormatter('%B %Y')\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    \n",
    "    # Rotate and align the tick labels\n",
    "    ax.tick_params(axis='x', labelsize=15)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"dtw_alignment_{metric_type.lower()}_{source.lower()}_{threshold}_{window}_{comparison.lower()}_r{radius}.png\"\n",
    "    plt.savefig(filename.replace('&', 'and'), bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_network_metrics(base_path: str, disease_dfs: Dict[str, pd.DataFrame], \n",
    "                          radii: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze network metrics against disease cases\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Process network density files\n",
    "    density_path = os.path.join(base_path, \"gt_netdense_rsvmsv_15or30day\")\n",
    "    pattern = \"netdense_*.csv\"\n",
    "    density_files = glob.glob(os.path.join(density_path, pattern))\n",
    "    \n",
    "    for file_path in density_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        _, source, threshold, window = filename.replace('.csv', '').split('_')\n",
    "        window = window.replace('day', '')\n",
    "        \n",
    "        print(f\"\\nProcessing Network Density: {filename}\")\n",
    "        \n",
    "        # Load network density data\n",
    "        metric_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Skip if all zeros\n",
    "        if np.all(metric_df['network_density'] == 0):\n",
    "            print(f\"Skipping {filename} - contains all zeros\")\n",
    "            continue\n",
    "            \n",
    "        metric_df['date'] = pd.to_datetime(metric_df['date'])\n",
    "        \n",
    "        # Compare with each disease metric\n",
    "        for disease_name, disease_df in disease_dfs.items():\n",
    "            merged_df = pd.merge(metric_df, disease_df, on='date', how='inner')\n",
    "            \n",
    "            if len(merged_df) == 0:\n",
    "                print(f\"No overlapping dates found for {filename} and {disease_name}\")\n",
    "                continue\n",
    "                \n",
    "            metric_series = merged_df['network_density'].values\n",
    "            disease_series = merged_df.iloc[:, -1].values\n",
    "            dates = merged_df['date'].values\n",
    "            \n",
    "            # Compute DTW for each radius\n",
    "            for radius in radii:\n",
    "                try:\n",
    "                    dtw_score, _ = compute_dtw_with_temporal_constraint(\n",
    "                        disease_series, metric_series, radius\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        'metric_type': 'Network Density',\n",
    "                        'source': source.upper(),\n",
    "                        'threshold': threshold,\n",
    "                        'window': window,\n",
    "                        'comparison': disease_name,\n",
    "                        'radius': radius,\n",
    "                        'dtw_score': dtw_score\n",
    "                    })\n",
    "                    \n",
    "                    # Generate visualization\n",
    "                    plot_dtw_alignment(disease_series, metric_series, dates,\n",
    "                                     'Network Density', 'Network Density', source.upper(),\n",
    "                                     threshold, window, disease_name, radius)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename} with radius {radius}: {str(e)}\")\n",
    "    \n",
    "    # Process clustering coefficient files\n",
    "    coeff_path = os.path.join(base_path, \"gt_cluscoeff_rsvmsv_15or30day\")\n",
    "    pattern = \"cluscoeff_*.csv\"\n",
    "    coeff_files = glob.glob(os.path.join(coeff_path, pattern))\n",
    "    \n",
    "    for file_path in coeff_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        _, source, threshold, window = filename.replace('.csv', '').split('_')\n",
    "        window = window.replace('day', '')\n",
    "        \n",
    "        print(f\"\\nProcessing Clustering Coefficient: {filename}\")\n",
    "        \n",
    "        # Load clustering coefficient data\n",
    "        metric_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Skip if all zeros\n",
    "        if np.all(metric_df['clustering_coefficient'] == 0):\n",
    "            print(f\"Skipping {filename} - contains all zeros\")\n",
    "            continue\n",
    "            \n",
    "        metric_df['date'] = pd.to_datetime(metric_df['date'])\n",
    "        \n",
    "        # Compare with each disease metric\n",
    "        for disease_name, disease_df in disease_dfs.items():\n",
    "            merged_df = pd.merge(metric_df, disease_df, on='date', how='inner')\n",
    "            \n",
    "            if len(merged_df) == 0:\n",
    "                print(f\"No overlapping dates found for {filename} and {disease_name}\")\n",
    "                continue\n",
    "                \n",
    "            metric_series = merged_df['clustering_coefficient'].values\n",
    "            disease_series = merged_df.iloc[:, -1].values\n",
    "            dates = merged_df['date'].values\n",
    "            \n",
    "            # Compute DTW for each radius\n",
    "            for radius in radii:\n",
    "                try:\n",
    "                    dtw_score, _ = compute_dtw_with_temporal_constraint(\n",
    "                        disease_series, metric_series, radius\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        'metric_type': 'Clustering Coefficient',\n",
    "                        'source': source.upper(),\n",
    "                        'threshold': threshold,\n",
    "                        'window': window,\n",
    "                        'comparison': disease_name,\n",
    "                        'radius': radius,\n",
    "                        'dtw_score': dtw_score\n",
    "                    })\n",
    "                    \n",
    "                    # Generate visualization\n",
    "                    plot_dtw_alignment(disease_series, metric_series, dates,\n",
    "                                     'Clustering Coefficient', 'Clustering Coefficient',\n",
    "                                     source.upper(), threshold, window, disease_name, radius)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename} with radius {radius}: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load disease data\n",
    "confirmed_df = pd.read_csv(\"../gt_stat_analysis/disease_confirmed_daily_cases.csv\")\n",
    "active_df = pd.read_csv(\"../gt_stat_analysis/disease_active_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to datetime\n",
    "confirmed_df['date'] = pd.to_datetime(confirmed_df['date'])\n",
    "active_df['date'] = pd.to_datetime(active_df['date'])\n",
    "\n",
    "# Create disease dataframes dictionary\n",
    "disease_dfs = {\n",
    "    'Confirmed Cases': confirmed_df,\n",
    "    'Active Cases': active_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define radii for analysis\n",
    "radii = [7, 15, 20, 30, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path for network metrics\n",
    "base_path = \"../gt_netdense_cluscoeff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing network metrics\n",
      "\n",
      "Processing Network Density: netdense_msv_0.5_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.8_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.6_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.4_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.4_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.6_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.8_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.5_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.8_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.5_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.4_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.6_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.6_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_msv_0.4_15day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.5_30day.csv\n",
      "\n",
      "Processing Network Density: netdense_rsv_0.8_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.8_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.5_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.4_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.6_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.6_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.4_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.5_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.8_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.5_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.8_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.6_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.4_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.4_15day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_msv_0.6_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.8_30day.csv\n",
      "\n",
      "Processing Clustering Coefficient: cluscoeff_rsv_0.5_30day.csv\n"
     ]
    }
   ],
   "source": [
    "# Analyze network metrics\n",
    "print(\"Processing network metrics\")\n",
    "all_results = analyze_network_metrics(base_path, disease_dfs, radii)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
