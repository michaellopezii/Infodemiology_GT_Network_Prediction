{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_constraint_mask(length: int, radius: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a mask matrix for temporal constraints.\n",
    "    Points outside the radius window will be masked with True\n",
    "    \"\"\"\n",
    "    mask = np.zeros((length, length), dtype=bool)\n",
    "    \n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if abs(i - j) > radius:\n",
    "                mask[i, j] = True\n",
    "                \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dtw_with_temporal_constraint(series1: np.ndarray, series2: np.ndarray, radius: int) -> Tuple[float, List]:\n",
    "    \"\"\"\n",
    "    Compute DTW with strict temporal constraints\n",
    "    \"\"\"\n",
    "    # Normalize series to [0,1] range\n",
    "    s1_norm = (series1 - np.min(series1)) / (np.max(series1) - np.min(series1))\n",
    "    s2_norm = (series2 - np.min(series2)) / (np.max(series2) - np.min(series2))\n",
    "    \n",
    "    n, m = len(s1_norm), len(s2_norm)\n",
    "    \n",
    "    # Create cost matrix\n",
    "    cost_matrix = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            cost_matrix[i, j] = abs(s1_norm[i] - s2_norm[j])\n",
    "    \n",
    "    # Create accumulated cost matrix with temporal constraint\n",
    "    D = np.full((n, m), np.inf)\n",
    "    D[0, 0] = cost_matrix[0, 0]\n",
    "    \n",
    "    # Create temporal constraint mask\n",
    "    temporal_mask = create_temporal_constraint_mask(max(n, m), radius)\n",
    "    temporal_mask = temporal_mask[:n, :m]\n",
    "    \n",
    "    # Apply temporal constraint\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if temporal_mask[i, j]:\n",
    "                continue\n",
    "                \n",
    "            if i == 0 and j == 0:\n",
    "                continue\n",
    "                \n",
    "            candidates = []\n",
    "            if i > 0:\n",
    "                candidates.append(D[i-1, j])\n",
    "            if j > 0:\n",
    "                candidates.append(D[i, j-1])\n",
    "            if i > 0 and j > 0:\n",
    "                candidates.append(D[i-1, j-1])\n",
    "            \n",
    "            if candidates:\n",
    "                D[i, j] = cost_matrix[i, j] + min(candidates)\n",
    "    \n",
    "    # Backtrack to find the warping path\n",
    "    path = []\n",
    "    i, j = n-1, m-1\n",
    "    path.append((i, j))\n",
    "    \n",
    "    while i > 0 or j > 0:\n",
    "        candidates = []\n",
    "        if i > 0:\n",
    "            candidates.append((D[i-1, j], i-1, j))\n",
    "        if j > 0:\n",
    "            candidates.append((D[i, j-1], i, j-1))\n",
    "        if i > 0 and j > 0:\n",
    "            candidates.append((D[i-1, j-1], i-1, j-1))\n",
    "            \n",
    "        _, i, j = min(candidates, key=lambda x: x[0])\n",
    "        path.append((i, j))\n",
    "    \n",
    "    path.reverse()\n",
    "    \n",
    "    return D[-1, -1], path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dtw_alignment(series1: np.ndarray, series2: np.ndarray, dates: np.ndarray,\n",
    "                      metric_type: str, pca_group: str, threshold: str, window: str,\n",
    "                      comparison: str, radius: int) -> None:\n",
    "    \"\"\"\n",
    "    Plot the DTW alignment between PCA group metrics and disease cases\n",
    "    \"\"\"\n",
    "    # Normalize series for visualization\n",
    "    s1_norm = (series1 - np.min(series1)) / (np.max(series1) - np.min(series1))\n",
    "    s2_norm = (series2 - np.min(series2)) / (np.max(series2) - np.min(series2))\n",
    "    \n",
    "    # Compute DTW with temporal constraint\n",
    "    distance, path = compute_dtw_with_temporal_constraint(series1, series2, radius)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    # Plot the first time series at the top\n",
    "    ax.plot(dates, s1_norm + 1.5, label=f'{comparison}', color='blue', linewidth=2)\n",
    "    \n",
    "    # Plot the second time series at the bottom\n",
    "    metric_label = f\"{metric_type} ({pca_group}, thresh={threshold}, window={window})\"\n",
    "    ax.plot(dates, s2_norm, label=metric_label, color='red', linewidth=2)\n",
    "    \n",
    "    # Draw matching lines between points\n",
    "    path = np.array(path)\n",
    "    for i, j in path[::3]:  # Plot every 3rd line to reduce visual clutter\n",
    "        ax.plot([dates[i], dates[j]], [s1_norm[i] + 1.5, s2_norm[j]], \n",
    "                'gray', alpha=0.9, linestyle='--')\n",
    "    \n",
    "    # Customize the plot\n",
    "    title = f'DTW Alignment (Â±{radius} days): {pca_group}\\n{metric_type} vs {comparison}'\n",
    "    ax.set_title(title, fontsize=25, pad=20)\n",
    "    ax.legend(fontsize=13, loc='upper right')\n",
    "    \n",
    "    # Remove y-axis ticks and labels\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Format x-axis dates\n",
    "    formatter = mdates.DateFormatter('%B %Y')\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    \n",
    "    # Rotate and align the tick labels\n",
    "    ax.tick_params(axis='x', labelsize=15)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"dtw_alignment_{pca_group}_{metric_type.lower()}_{threshold}_{window}_{comparison.lower()}_r{radius}.png\"\n",
    "    plt.savefig(filename.replace('&', 'and'), bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pca_group_metrics(base_path: str, disease_dfs: Dict[str, pd.DataFrame], \n",
    "                            radii: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze PCA group metrics against disease cases\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get all PCA group directories\n",
    "    pca_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and d.startswith('pc')]\n",
    "    \n",
    "    for pca_dir in pca_dirs:\n",
    "        # Get group name without pc#_ prefix\n",
    "        group_name = '_'.join(pca_dir.split('_')[1:])\n",
    "        dir_path = os.path.join(base_path, pca_dir)\n",
    "        \n",
    "        print(f\"\\nProcessing PCA group: {group_name}\")\n",
    "        \n",
    "        # Process network density files\n",
    "        density_pattern = os.path.join(dir_path, \"netdense_*.csv\")\n",
    "        for file_path in glob.glob(density_pattern):\n",
    "            filename = os.path.basename(file_path)\n",
    "            thresh = filename.split('threshold_')[0].split('_')[-1]\n",
    "            window = filename.split('threshold_')[1].split('.')[0]\n",
    "            \n",
    "            # Load network density data\n",
    "            metric_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Skip if all zeros\n",
    "            if np.all(metric_df['network_density'] == 0):\n",
    "                print(f\"Skipping {filename} - network density contains all zeros\")\n",
    "                continue\n",
    "                \n",
    "            metric_df['date'] = pd.to_datetime(metric_df['date'])\n",
    "            \n",
    "            # Compare with each disease metric\n",
    "            for disease_name, disease_df in disease_dfs.items():\n",
    "                merged_df = pd.merge(metric_df, disease_df, on='date', how='inner')\n",
    "                \n",
    "                if len(merged_df) == 0:\n",
    "                    print(f\"No overlapping dates found for {filename} and {disease_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                metric_series = merged_df['network_density'].values\n",
    "                disease_series = merged_df.iloc[:, -1].values\n",
    "                dates = merged_df['date'].values\n",
    "                \n",
    "                # Compute DTW for each radius\n",
    "                for radius in radii:\n",
    "                    try:\n",
    "                        dtw_score, _ = compute_dtw_with_temporal_constraint(\n",
    "                            disease_series, metric_series, radius\n",
    "                        )\n",
    "                        \n",
    "                        results.append({\n",
    "                            'pca_group': group_name,\n",
    "                            'metric_type': 'Network Density',\n",
    "                            'threshold': thresh,\n",
    "                            'window': window,\n",
    "                            'comparison': disease_name,\n",
    "                            'radius': radius,\n",
    "                            'dtw_score': dtw_score\n",
    "                        })\n",
    "                        \n",
    "                        # Generate visualization\n",
    "                        plot_dtw_alignment(disease_series, metric_series, dates,\n",
    "                                         'Network Density', group_name, thresh,\n",
    "                                         window, disease_name, radius)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {filename} with radius {radius}: {str(e)}\")\n",
    "        \n",
    "        # Process clustering coefficient files\n",
    "        coeff_pattern = os.path.join(dir_path, \"cluscoeff_*.csv\")\n",
    "        for file_path in glob.glob(coeff_pattern):\n",
    "            filename = os.path.basename(file_path)\n",
    "            thresh = filename.split('threshold_')[0].split('_')[-1]\n",
    "            window = filename.split('threshold_')[1].split('.')[0]\n",
    "            \n",
    "            # Load clustering coefficient data\n",
    "            metric_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Skip if all zeros\n",
    "            if np.all(metric_df['clustering_coefficient'] == 0):\n",
    "                print(f\"Skipping {filename} - clustering coefficient contains all zeros\")\n",
    "                continue\n",
    "                \n",
    "            metric_df['date'] = pd.to_datetime(metric_df['date'])\n",
    "            \n",
    "            # Compare with each disease metric\n",
    "            for disease_name, disease_df in disease_dfs.items():\n",
    "                merged_df = pd.merge(metric_df, disease_df, on='date', how='inner')\n",
    "                \n",
    "                if len(merged_df) == 0:\n",
    "                    print(f\"No overlapping dates found for {filename} and {disease_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                metric_series = merged_df['clustering_coefficient'].values\n",
    "                disease_series = merged_df.iloc[:, -1].values\n",
    "                dates = merged_df['date'].values\n",
    "                \n",
    "                # Compute DTW for each radius\n",
    "                for radius in radii:\n",
    "                    try:\n",
    "                        dtw_score, _ = compute_dtw_with_temporal_constraint(\n",
    "                            disease_series, metric_series, radius\n",
    "                        )\n",
    "                        \n",
    "                        results.append({\n",
    "                            'pca_group': group_name,\n",
    "                            'metric_type': 'Clustering Coefficient',\n",
    "                            'threshold': thresh,\n",
    "                            'window': window,\n",
    "                            'comparison': disease_name,\n",
    "                            'radius': radius,\n",
    "                            'dtw_score': dtw_score\n",
    "                        })\n",
    "                        \n",
    "                        # Generate visualization\n",
    "                        plot_dtw_alignment(disease_series, metric_series, dates,\n",
    "                                         'Clustering Coefficient', group_name, thresh,\n",
    "                                         window, disease_name, radius)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {filename} with radius {radius}: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load disease data\n",
    "confirmed_df = pd.read_csv(\"../gt_stat_analysis/disease_confirmed_daily_cases.csv\")\n",
    "active_df = pd.read_csv(\"../gt_stat_analysis/disease_active_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df['date'] = pd.to_datetime(confirmed_df['date'])\n",
    "active_df['date'] = pd.to_datetime(active_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_dfs = {\n",
    "    'Confirmed Cases': confirmed_df,\n",
    "    'Active Cases': active_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define radii for analysis\n",
    "radii = [7, 15, 20, 30, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../gt_pca_corr_adj_matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PCA group metrics\n",
      "\n",
      "Processing PCA group: MSVFaceWearing&Others-0.5\n",
      "Skipping netdense_MSVFaceWearing&Others-0.5_0.8threshold_30day.csv - network density contains all zeros\n",
      "Skipping cluscoeff_MSVFaceWearing&Others-0.5_0.8threshold_15day.csv - clustering coefficient contains all zeros\n",
      "Skipping cluscoeff_MSVFaceWearing&Others-0.5_0.8threshold_30day.csv - clustering coefficient contains all zeros\n",
      "Skipping cluscoeff_MSVFaceWearing&Others-0.5_0.6threshold_30day.csv - clustering coefficient contains all zeros\n",
      "\n",
      "Processing PCA group: RSVSymptoms&NewNormalProtocols1-0.6\n",
      "\n",
      "Processing PCA group: RSVFaceWearing&Others-0.5\n",
      "Skipping netdense_RSVFaceWearing&Others-0.5_0.8threshold_30day.csv - network density contains all zeros\n",
      "Skipping cluscoeff_RSVFaceWearing&Others-0.5_0.8threshold_15day.csv - clustering coefficient contains all zeros\n",
      "Skipping cluscoeff_RSVFaceWearing&Others-0.5_0.6threshold_30day.csv - clustering coefficient contains all zeros\n",
      "Skipping cluscoeff_RSVFaceWearing&Others-0.5_0.8threshold_30day.csv - clustering coefficient contains all zeros\n",
      "\n",
      "Processing PCA group: MSVSymptoms-0.5\n",
      "\n",
      "Processing PCA group: RSVSymptoms&NewNormalProtocols-0.5\n",
      "\n",
      "Processing PCA group: RSVSymptoms&NewNormalProtocols2-0.6\n",
      "Skipping netdense_RSVSymptoms&NewNormalProtocols2-0.6_0.8threshold_30day.csv - network density contains all zeros\n",
      "Skipping cluscoeff_RSVSymptoms&NewNormalProtocols2-0.6_0.6threshold_30day.csv - clustering coefficient contains all zeros\n",
      "Skipping cluscoeff_RSVSymptoms&NewNormalProtocols2-0.6_0.8threshold_30day.csv - clustering coefficient contains all zeros\n",
      "Skipping cluscoeff_RSVSymptoms&NewNormalProtocols2-0.6_0.8threshold_15day.csv - clustering coefficient contains all zeros\n",
      "\n",
      "Processing PCA group: MSVSymptoms&NewNormalProtocols-0.6\n",
      "Skipping cluscoeff_MSVSymptoms&NewNormalProtocols-0.6_0.8threshold_30day.csv - clustering coefficient contains all zeros\n",
      "Skipping cluscoeff_MSVSymptoms&NewNormalProtocols-0.6_0.8threshold_15day.csv - clustering coefficient contains all zeros\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing PCA group metrics\")\n",
    "all_results = analyze_pca_group_metrics(base_path, disease_dfs, radii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results sorted by radius and DTW score\n",
    "results_sorted = all_results.sort_values(['radius', 'dtw_score'])\n",
    "results_sorted.to_csv('pca_dtw_results_multi_radius.csv', index=False)\n",
    "\n",
    "# Save results sorted only by DTW score\n",
    "results_sorted_by_score = all_results.sort_values('dtw_score', ascending=True)\n",
    "results_sorted_by_score.to_csv('pca_dtw_results_sorted.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
